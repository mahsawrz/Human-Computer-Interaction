# -*- coding: utf-8 -*-
"""HCI_Rezaei_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11GiWd8852mmkeJQDWZ8R1DWf1FXdL0yZ
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report
import numpy as np

# Load the dataset
df = pd.read_csv("bank-full.csv", delimiter=';')

# Encode categorical variables
df_encoded = pd.get_dummies(df, columns=['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome'])

# Map 'yes' and 'no' to 1 and 0 for the target variable
df_encoded['y'] = df_encoded['y'].map({'yes': 1, 'no': 0})

# Separate features and labels
X = df_encoded.drop('y', axis=1)
y = df_encoded['y']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Balance the dataset using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

# Split the dataset into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Convert labels to one-hot encoding for neural network
y_train_one_hot = to_categorical(y_train)
y_test_one_hot = to_categorical(y_test)

# Build a simple neural network
nn_model = Sequential()
nn_model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))
nn_model.add(Dense(32, activation='relu'))
nn_model.add(Dense(2, activation='softmax'))  # Adjusted output layer for binary classification

# Function to get predicted probabilities from the neural network model
def predict_proba_nn(nn_model, instance):
    return nn_model.predict(instance.reshape(1, -1))[0]

# Compile the model
nn_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the neural network
nn_model.fit(X_train, y_train_one_hot, epochs=10, batch_size=64, validation_split=0.1, verbose=1)

# Evaluate the model on the test set
y_pred_proba = nn_model.predict(X_test)
y_pred = y_pred_proba.argmax(axis=1)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred) * 100
print(f"\nAccuracy: {accuracy:.2f}%")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# Load the dataset
df = pd.read_csv("bank.csv", delimiter=';')

# Encode categorical variables
df_encoded = pd.get_dummies(df, columns=['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome'])

# Map 'yes' and 'no' to 1 and 0 for the target variable
df_encoded['y'] = df_encoded['y'].map({'yes': 1, 'no': 0})

# Separate features and labels
X = df_encoded.drop('y', axis=1)
y = df_encoded['y']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Balance the dataset using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

# Split the dataset into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Build a Support Vector Machine (SVM) model
svm_model = SVC(kernel='linear', random_state=42,probability=True)

# Train the SVM model
svm_model.fit(X_train, y_train)

# Evaluate the model on the test set
y_pred = svm_model.predict(X_test)

# Print accuracy and classification report
accuracy = accuracy_score(y_test, y_pred) * 100
print(f"\nAccuracy: {accuracy:.2f}%")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# Load the dataset
df = pd.read_csv("bank-full.csv", delimiter=';')

# Encode categorical variables
df_encoded = pd.get_dummies(df, columns=['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome'])

# Map 'yes' and 'no' to 1 and 0 for the target variable
df_encoded['y'] = df_encoded['y'].map({'yes': 1, 'no': 0})

# Separate features and labels
X = df_encoded.drop('y', axis=1)
y = df_encoded['y']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Balance the dataset using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

# Split the dataset into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Build a Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the Random Forest model
rf_model.fit(X_train, y_train)

# Evaluate the model on the test set
y_pred = rf_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred) * 100
print(f"\nAccuracy: {accuracy:.2f}%")

!pip install lime
!pip install shap
!pip install ipywidgets shap lime

import pandas as pd
import numpy as np
import ipywidgets as widgets
from ipywidgets import HBox, VBox, Layout
from IPython.display import display, clear_output
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from lime.lime_tabular import LimeTabularExplainer
import shap
import matplotlib.pyplot as plt

# color scheme for the widgets
widget_color = '#00CED1'

# Lime explanation
explainer_lime_nn = LimeTabularExplainer(training_data=X_train, feature_names=X.columns, class_names=['No', 'Yes'], mode='classification')
explainer_lime_rf = LimeTabularExplainer(training_data=X_train, feature_names=X.columns, class_names=['No', 'Yes'], mode='classification')
explainer_lime_svm = LimeTabularExplainer(training_data=X_train, feature_names=X.columns, class_names=['No', 'Yes'], mode='classification')

# SHAP explanation
explainer_shap = shap.Explainer(nn_model, X_train)

# Counterfactual method
def generate_counterfactual(instance, model):
    # Create a copy of the instance
    counterfactual_instance = instance.copy()

    # Manipulate the instance features with random variations
    for feature in range(len(instance)):
        # Add a random value between -0.1 and 0.1 to each feature
        counterfactual_instance[feature] = instance[feature] + np.random.uniform(low=-0.1, high=0.1)

    return counterfactual_instance

# Explanation methods
explanation_methods = ['LIME', 'SHAP', 'Counterfactual']

# Checkbox for selecting whether to choose a sample
sample_checkbox = widgets.Checkbox(
    value=False,
    description='Choose Sample',
    style={'description_width': 'initial'},
    layout=Layout(width='auto', margin='0 0 10px 0')
)

# Combine the checkbox and description in an HBox
# The checkbox and its associated description are displayed together horizontally.
checkbox_container = HBox([sample_checkbox])

# Slider for selecting sample indices (initially disabled)
sample_slider = widgets.IntSlider(
    value=0,
    min=0,
    max=len(X_test) - 1,
    description='Sample',
    disabled=True,
    style={'description_width': 'initial'},
    layout=Layout(width='40%')
)

# Dropdown menu for selecting the model
model_dropdown = widgets.Dropdown(
    options={'Neural Network': 'nn_model', 'Random Forest': 'rf_model', 'SVM': 'svm_model'},
    value='nn_model',
    description='Select Model'
)

# Dropdown menu for selecting the explanation method
method_dropdown = widgets.Dropdown(
    options=explanation_methods,
    value='LIME',
    description='Explanation'
)

# Output area for displaying sample descriptions
output_area = widgets.Output(
    layout={'border': '1px solid #00CED1', 'padding': '10px', 'margin': '20px 0 10px 0', 'margin-left': '10px 0'}
)

# Button for displaying SHAP description for the entire dataset
shap_button = widgets.Button(
    description="General description",
    style={'button_color': widget_color, 'font_weight': 'bold'},

)

# Display widgets in a more organized layout
ui_layout = VBox([
    checkbox_container,
    sample_slider,
    model_dropdown,
    method_dropdown,
    output_area,
    shap_button
])

# Display the UI
display(ui_layout)

# Function to update explanations based on dropdown and slider values
def update_explanations(change):
    if not sample_checkbox.value:
        return

    idx = sample_slider.value
    model = model_dropdown.value
    method = method_dropdown.value

    with output_area:
        clear_output(wait=True)

        if method == 'LIME':
            if model == 'nn_model':
                exp = explainer_lime_nn.explain_instance(X_test[idx], nn_model.predict, num_features=len(X.columns))
            elif model == 'rf_model':
                exp = explainer_lime_rf.explain_instance(X_test[idx], rf_model.predict_proba, num_features=len(X.columns), num_samples=len(X_test)-1)
            elif model == 'svm_model':
                exp = explainer_lime_svm.explain_instance(X_test[idx], svm_model.predict_proba, num_features=len(X.columns), num_samples=len(X_test)-1)

            # Text explanation for LIME
            print(f"\nLIME Explanation for Sample Index {idx} and Model {model}:")
            explanation_list = exp.as_list()
            for explanation in explanation_list:
                feature, value = explanation
                print(f"{feature}: {value:.4f}")

            # Draw Lime explanation graph with clear y-axis labels using horizontal bar chart annotations
            fig, ax = plt.subplots(figsize=(12, 8))
            values = [val for feature, val in explanation_list]
            features = [feature for feature, val in explanation_list]
            y_pos = range(len(features))
            ax.barh(y_pos, values, align='center')
            ax.set_yticks(y_pos)
            ax.set_yticklabels(features)
            ax.invert_yaxis()  # Invert y axis to have the highest value on top
            ax.set_xlabel('LIME Value')
            ax.set_title('LIME Explanation Graph')
            plt.xlim([min(values) - 0.1, max(values) + 0.1])
            plt.show()
        elif method == 'SHAP':
            exp = explainer_shap.shap_values(X_test[idx].reshape(1, -1))
            print(f"\nSHAP Explanation for Sample Index {idx}:")

            # Numerical explanation
            print(exp)
            # Extract SHAP values for the positive class (assuming binary classification)
            shap_values_positive_class = exp[0][:, 1]

            # Draw SHAP explanation graph as a horizontal bar chart
            fig, ax = plt.subplots(figsize=(12, 8))
            features = X.columns
            y_pos = range(len(features))

            # Ensure that both y_pos and shap_values_positive_class have the same length
            if len(y_pos) != len(shap_values_positive_class):
                raise ValueError("Length mismatch between y_pos and shap_values_positive_class")

            ax.barh(y_pos, shap_values_positive_class, align='center')
            ax.set_yticks(y_pos)
            ax.set_yticklabels(features)
            ax.invert_yaxis()  # Invert y axis to have the highest value on top
            ax.set_xlabel('SHAP Value')
            ax.set_title('SHAP Explanation Graph')
            plt.show()
        elif method == 'Counterfactual':
            instance = X_test[idx]
            counterfactual_instance = generate_counterfactual(instance, model)
            print(f"\nCounterfactuals for Sample Index {idx}:")
            print("\nOriginal Instance:")
            print(instance)
            print("\nCounterfactual Instance:")
            print(counterfactual_instance)

            # Draw Counterfactual explanation graph (assuming features are numeric)
            fig, ax = plt.subplots(figsize=(12, 8))
            ax.barh(range(len(instance)), instance, color='#00CED1', label='Original')
            ax.barh(range(len(counterfactual_instance)), counterfactual_instance, color='pink', label='Counterfactual')
            ax.set_yticks(range(len(instance)))
            ax.set_yticklabels(X.columns)
            ax.invert_yaxis()
            ax.set_xlabel('Feature Value')
            ax.set_title('Counterfactual Explanation Graph')
            ax.legend()
            plt.show()

# Attach the function to the dropdown and slider's value change events
model_dropdown.observe(update_explanations, names='value')
method_dropdown.observe(update_explanations, names='value')
sample_slider.observe(update_explanations, names='value')

# Use a regular function to update the disabled state of the sample slider
def update_sample_slider_disabled(change):
    sample_slider.disabled = not sample_checkbox.value

sample_checkbox.observe(update_sample_slider_disabled, names='value')

# Function to generate and display SHAP explanations for the entire dataset
def show_shap_description(button_click):
    model = model_dropdown.value
    method = 'SHAP'

    # Display general description using SHAP method
    shap_values_summary = explainer_shap.shap_values(X_test)
    # Display numerical summary
    print(f"\nSHAP General description of {model_dropdown.label}:")
    print(shap_values_summary)

# Link the button to the function
shap_button.on_click(show_shap_description)